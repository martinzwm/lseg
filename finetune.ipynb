{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='../datasets', accumulate_grad_batches=2, max_epochs=240, dataset='ade20k', batch_size=1, base_lr=0.004, momentum=0.9, weight_decay=0.0001, aux=False, aux_weight=0.2, se_loss=False, se_weight=0.2, midasproto=False, ignore_index=-1, augment=False, backbone='clip_vitl16_384', num_features=256, dropout=0.1, finetune_weights=None, no_scaleinv=True, no_batchnorm=False, widehead=False, widehead_hr=False, arch_option=0, block_depth=0, activation='lrelu', exp_name='lseg_ade20k_l16')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, help=\"path where dataset is stored\", default=\"../datasets\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accumulate_grad_batches\", type=int, help=\"\", default=2\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_epochs\", type=int, help=\"max epochs\", default=240\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset\",\n",
    "    default=\"ade20k\",\n",
    "    help=\"dataset to train on\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=1, help=\"size of the batches\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--base_lr\", type=float, default=0.004, help=\"learning rate\"\n",
    ")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"SGD momentum\")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\", type=float, default=1e-4, help=\"weight_decay\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux\", action=\"store_true\", default=False, help=\"Auxilary Loss\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux-weight\",\n",
    "    type=float,\n",
    "    default=0.2,\n",
    "    help=\"Auxilary loss weight (default: 0.2)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-loss\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Semantic Encoding Loss SE-loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-weight\", type=float, default=0.2, help=\"SE-loss weight (default: 0.2)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--midasproto\", action=\"store_true\", default=False, help=\"midasprotocol\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ignore_index\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"numeric value of ignore label in gt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--augment\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Use extended augmentations\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--backbone\",\n",
    "    type=str,\n",
    "    default=\"clip_vitl16_384\",\n",
    "    help=\"backbone network\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_features\",\n",
    "    type=int,\n",
    "    default=256,\n",
    "    help=\"number of featurs that go from encoder to decoder\",\n",
    ")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout rate\")\n",
    "parser.add_argument(\n",
    "    \"--finetune_weights\", type=str, help=\"load weights to finetune from\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-scaleinv\",\n",
    "    default=True,\n",
    "    action=\"store_false\",\n",
    "    help=\"turn off scaleinv layers\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-batchnorm\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"turn off batchnorm\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead\", default=False, action=\"store_true\", help=\"wider output head\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead_hr\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"wider output head\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arch_option\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"which kind of architecture to be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--block_depth\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"how many blocks should be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--activation\",\n",
    "    choices=['lrelu', 'tanh'],\n",
    "    default=\"lrelu\",\n",
    "    help=\"use which activation to activate the block\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"lseg_ade20k_l16\"\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_training(args, LSegModule)\n",
    "from modules.lseg_module import LSegModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils import make_checkpoint_callbacks, get_wandb_logger\n",
    "\n",
    "checkpoint = \"./checkpoints/demo_e200.ckpt\"\n",
    "args.lr = 0.00001\n",
    "\n",
    "lseg = LSegModule.load_from_checkpoint(checkpoint, **vars(args))\n",
    "\n",
    "# set all sorts of training parameters\n",
    "args.gpus = -1\n",
    "args.accelerator = \"ddp\"\n",
    "args.benchmark = True\n",
    "\n",
    "args.version = 0\n",
    "\n",
    "args.sync_batchnorm = True\n",
    "\n",
    "ttlogger = pl.loggers.TestTubeLogger(\n",
    "    \"checkpoints\", name=args.exp_name, version=args.version\n",
    ")\n",
    "\n",
    "args.callbacks = make_checkpoint_callbacks(args.exp_name, args.version)\n",
    "\n",
    "wblogger = get_wandb_logger(args)\n",
    "args.logger = [wblogger, ttlogger]\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args)\n",
    "\n",
    "# only train on a subset of data during dev\n",
    "trainer.limit_train_batches = 0.001\n",
    "trainer.limit_val_batches = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_biomed_clip(device):\n",
    "    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    model.to(device)\n",
    "    return model, tokenizer, preprocess_train, preprocess_val\n",
    "\n",
    "# define the LightningModule\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, lseg):\n",
    "        super().__init__()\n",
    "        self.lseg = lseg\n",
    "        # self.biomed_clip_model, self.biomed_clip_tokenizer, self.preprocess_train, self.preprocess_val = load_biomed_clip(device)\n",
    "        # image_features, text_features, logit_scale = biomed_clip_model(images, texts)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # loader_lseg = self.lseg.train_dataloader()\n",
    "        # loader_biomed_clip = None # load the biomed clip data\n",
    "\n",
    "        # return {\"lseg\": loader_lseg, \"biomed_clip\": loader_biomed_clip}\n",
    "        return self.lseg.train_dataloader()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # # access a dictionary with a batch from each DataLoader\n",
    "        # batch_lseg = batch[\"lseg\"]\n",
    "        # batch_biomed_clip = batch[\"biomed_clip\"]\n",
    "\n",
    "        # seg_loss = self.lseg.training_step(batch_lseg, batch_idx)\n",
    "        # return seg_loss\n",
    "        # # adapt_loss = adapt_loss(batch_biomed_clip)\n",
    "        return self.lseg.training_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.lseg.configure_optimizers()\n",
    "\n",
    "    # def adapt_loss(self, batch_biomed_clip):\n",
    "    #     # get the image and text features from biomed clip\n",
    "    #     # get the image and text features from lseg\n",
    "    #     # compute the loss between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(lseg)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
