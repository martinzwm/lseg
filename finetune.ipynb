{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='../datasets', accumulate_grad_batches=2, max_epochs=240, dataset='ade20k', batch_size=1, base_lr=0.004, momentum=0.9, weight_decay=0.0001, aux=False, aux_weight=0.2, se_loss=False, se_weight=0.2, midasproto=False, ignore_index=-1, augment=False, backbone='clip_vitl16_384', num_features=256, dropout=0.1, finetune_weights=None, no_scaleinv=True, no_batchnorm=False, widehead=False, widehead_hr=False, arch_option=0, block_depth=0, activation='lrelu', exp_name='lseg_ade20k_l16', project_name='testing')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, help=\"path where dataset is stored\", default=\"../datasets\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accumulate_grad_batches\", type=int, help=\"\", default=2\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_epochs\", type=int, help=\"max epochs\", default=240\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset\",\n",
    "    default=\"ade20k\",\n",
    "    help=\"dataset to train on\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=1, help=\"size of the batches\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--base_lr\", type=float, default=0.004, help=\"learning rate\"\n",
    ")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"SGD momentum\")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\", type=float, default=1e-4, help=\"weight_decay\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux\", action=\"store_true\", default=False, help=\"Auxilary Loss\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux-weight\",\n",
    "    type=float,\n",
    "    default=0.2,\n",
    "    help=\"Auxilary loss weight (default: 0.2)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-loss\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Semantic Encoding Loss SE-loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-weight\", type=float, default=0.2, help=\"SE-loss weight (default: 0.2)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--midasproto\", action=\"store_true\", default=False, help=\"midasprotocol\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ignore_index\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"numeric value of ignore label in gt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--augment\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Use extended augmentations\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--backbone\",\n",
    "    type=str,\n",
    "    default=\"clip_vitl16_384\",\n",
    "    help=\"backbone network\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_features\",\n",
    "    type=int,\n",
    "    default=256,\n",
    "    help=\"number of featurs that go from encoder to decoder\",\n",
    ")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout rate\")\n",
    "parser.add_argument(\n",
    "    \"--finetune_weights\", type=str, help=\"load weights to finetune from\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-scaleinv\",\n",
    "    default=True,\n",
    "    action=\"store_false\",\n",
    "    help=\"turn off scaleinv layers\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-batchnorm\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"turn off batchnorm\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead\", default=False, action=\"store_true\", help=\"wider output head\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead_hr\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"wider output head\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arch_option\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"which kind of architecture to be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--block_depth\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"how many blocks should be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--activation\",\n",
    "    choices=['lrelu', 'tanh'],\n",
    "    default=\"lrelu\",\n",
    "    help=\"use which activation to activate the block\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"lseg_ade20k_l16\"\n",
    "args.project_name = \"testing\"\n",
    "\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **\n",
      "{'base_size': 520, 'crop_size': 480}\n",
      "train\n",
      "BaseDataset: base_size 520, crop_size 480\n",
      "len(img_paths): 20210\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory checkpoints/lseg_ade20k_l16/version_0/checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:432: UserWarning: ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "Selected distributed backend ddp is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m wblogger \u001b[39m=\u001b[39m get_wandb_logger(args)\n\u001b[1;32m     28\u001b[0m args\u001b[39m.\u001b[39mlogger \u001b[39m=\u001b[39m [wblogger, ttlogger]\n\u001b[0;32m---> 30\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer\u001b[39m.\u001b[39;49mfrom_argparse_args(args)\n\u001b[1;32m     32\u001b[0m \u001b[39m# only train on a subset of data during dev\u001b[39;00m\n\u001b[1;32m     33\u001b[0m trainer\u001b[39m.\u001b[39mlimit_train_batches \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/properties.py:421\u001b[0m, in \u001b[0;36mTrainerProperties.from_argparse_args\u001b[0;34m(cls, args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_argparse_args\u001b[39m(\u001b[39mcls\u001b[39m: Type[\u001b[39m\"\u001b[39m\u001b[39m_T\u001b[39m\u001b[39m\"\u001b[39m], args: Union[Namespace, ArgumentParser], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_T\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 421\u001b[0m     \u001b[39mreturn\u001b[39;00m from_argparse_args(\u001b[39mcls\u001b[39;49m, args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/utilities/argparse.py:52\u001b[0m, in \u001b[0;36mfrom_argparse_args\u001b[0;34m(cls, args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m trainer_kwargs \u001b[39m=\u001b[39m {name: params[name] \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m valid_kwargs \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m params}\n\u001b[1;32m     50\u001b[0m trainer_kwargs\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrainer_kwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py:40\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     39\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:354\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, sync_batchnorm, precision, weights_summary, weights_save_path, num_sanity_val_steps, truncated_bptt_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, terminate_on_nan, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, distributed_backend, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[1;32m    352\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_connector \u001b[39m=\u001b[39m OptimizerConnector(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    355\u001b[0m     num_processes,\n\u001b[1;32m    356\u001b[0m     devices,\n\u001b[1;32m    357\u001b[0m     tpu_cores,\n\u001b[1;32m    358\u001b[0m     ipus,\n\u001b[1;32m    359\u001b[0m     distributed_backend,\n\u001b[1;32m    360\u001b[0m     accelerator,\n\u001b[1;32m    361\u001b[0m     gpus,\n\u001b[1;32m    362\u001b[0m     gpu_ids,\n\u001b[1;32m    363\u001b[0m     num_nodes,\n\u001b[1;32m    364\u001b[0m     sync_batchnorm,\n\u001b[1;32m    365\u001b[0m     benchmark,\n\u001b[1;32m    366\u001b[0m     replace_sampler_ddp,\n\u001b[1;32m    367\u001b[0m     deterministic,\n\u001b[1;32m    368\u001b[0m     precision,\n\u001b[1;32m    369\u001b[0m     amp_backend,\n\u001b[1;32m    370\u001b[0m     amp_level,\n\u001b[1;32m    371\u001b[0m     plugins,\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    373\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_connector \u001b[39m=\u001b[39m ModelConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:152\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, num_processes, devices, tpu_cores, ipus, distributed_backend, accelerator, gpus, gpu_ids, num_nodes, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, precision, amp_type, amp_level, plugins)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_if_devices_flag_ignored()\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_accelerator_type()\n\u001b[0;32m--> 152\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_distributed_mode()\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfigure_slurm_ddp()\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_given_plugins()\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:796\u001b[0m, in \u001b[0;36mAcceleratorConnector.set_distributed_mode\u001b[0;34m(self, distributed_backend)\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[39m# finished configuring self._distrib_type, check ipython environment\u001b[39;00m\n\u001b[0;32m--> 796\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_interactive_compatibility()\n\u001b[1;32m    798\u001b[0m \u001b[39m# for DDP overwrite nb processes by requested GPUs\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_device_type \u001b[39m==\u001b[39m DeviceType\u001b[39m.\u001b[39mGPU \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39min\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m     DistributedType\u001b[39m.\u001b[39mDDP,\n\u001b[1;32m    801\u001b[0m     DistributedType\u001b[39m.\u001b[39mDDP_SPAWN,\n\u001b[1;32m    802\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:840\u001b[0m, in \u001b[0;36mAcceleratorConnector.check_interactive_compatibility\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_INTERACTIVE\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m _IS_INTERACTIVE \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type\u001b[39m.\u001b[39mis_interactive_compatible():\n\u001b[0;32m--> 840\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    841\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSelected distributed backend \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distrib_type\u001b[39m}\u001b[39;00m\u001b[39m is not compatible with an interactive\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m environment. Run your code as a script, or choose one of the compatible backends:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(DistributedType\u001b[39m.\u001b[39minteractive_compatible_types())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: Selected distributed backend ddp is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible backends: dp, ddp_spawn, ddp_sharded_spawn, tpu_spawn"
     ]
    }
   ],
   "source": [
    "# do_training(args, LSegModule)\n",
    "from modules.lseg_module import LSegModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils import make_checkpoint_callbacks, get_wandb_logger\n",
    "\n",
    "checkpoint = \"./checkpoints/demo_e200.ckpt\"\n",
    "args.lr = 0.00001\n",
    "\n",
    "lseg = LSegModule.load_from_checkpoint(checkpoint, **vars(args))\n",
    "\n",
    "# set all sorts of training parameters\n",
    "args.gpus = -1\n",
    "args.accelerator = \"ddp_spawn\"\n",
    "args.benchmark = True\n",
    "\n",
    "args.version = 0\n",
    "\n",
    "args.sync_batchnorm = True\n",
    "\n",
    "ttlogger = pl.loggers.TestTubeLogger(\n",
    "    \"checkpoints\", name=args.exp_name, version=args.version\n",
    ")\n",
    "\n",
    "args.callbacks = make_checkpoint_callbacks(args.exp_name, args.version)\n",
    "\n",
    "wblogger = get_wandb_logger(args)\n",
    "args.logger = [wblogger, ttlogger]\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args)\n",
    "\n",
    "# only train on a subset of data during dev\n",
    "trainer.limit_train_batches = 0.001\n",
    "trainer.limit_val_batches = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_biomed_clip(device):\n",
    "    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    model.to(device)\n",
    "    return model, tokenizer, preprocess_train, preprocess_val\n",
    "\n",
    "# define the LightningModule\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, lseg):\n",
    "        super().__init__()\n",
    "        self.lseg = lseg\n",
    "        # self.biomed_clip_model, self.biomed_clip_tokenizer, self.preprocess_train, self.preprocess_val = load_biomed_clip(device)\n",
    "        # image_features, text_features, logit_scale = biomed_clip_model(images, texts)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # loader_lseg = self.lseg.train_dataloader()\n",
    "        # loader_biomed_clip = None # load the biomed clip data\n",
    "\n",
    "        # return {\"lseg\": loader_lseg, \"biomed_clip\": loader_biomed_clip}\n",
    "        return self.lseg.train_dataloader()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # # access a dictionary with a batch from each DataLoader\n",
    "        # batch_lseg = batch[\"lseg\"]\n",
    "        # batch_biomed_clip = batch[\"biomed_clip\"]\n",
    "\n",
    "        # seg_loss = self.lseg.training_step(batch_lseg, batch_idx)\n",
    "        # return seg_loss\n",
    "        # # adapt_loss = adapt_loss(batch_biomed_clip)\n",
    "        return self.lseg.training_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.lseg.configure_optimizers()\n",
    "\n",
    "    # def adapt_loss(self, batch_biomed_clip):\n",
    "    #     # get the image and text features from biomed clip\n",
    "    #     # get the image and text features from lseg\n",
    "    #     # compute the loss between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(lseg)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
