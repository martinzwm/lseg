{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='../datasets', accumulate_grad_batches=2, max_epochs=240, dataset='ade20k', batch_size=1, base_lr=0.004, momentum=0.9, weight_decay=0.0001, aux=False, aux_weight=0.2, se_loss=False, se_weight=0.2, midasproto=False, ignore_index=-1, augment=False, backbone='clip_vitl16_384', num_features=256, dropout=0.1, finetune_weights=None, no_scaleinv=True, no_batchnorm=False, widehead=False, widehead_hr=False, arch_option=0, block_depth=0, activation='lrelu', exp_name='lseg_ade20k_l16', project_name='testing')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(add_help=False)\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, help=\"path where dataset is stored\", default=\"../datasets\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--accumulate_grad_batches\", type=int, help=\"\", default=2\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_epochs\", type=int, help=\"max epochs\", default=240\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset\",\n",
    "    default=\"ade20k\",\n",
    "    help=\"dataset to train on\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=1, help=\"size of the batches\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--base_lr\", type=float, default=0.004, help=\"learning rate\"\n",
    ")\n",
    "parser.add_argument(\"--momentum\", type=float, default=0.9, help=\"SGD momentum\")\n",
    "parser.add_argument(\n",
    "    \"--weight_decay\", type=float, default=1e-4, help=\"weight_decay\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux\", action=\"store_true\", default=False, help=\"Auxilary Loss\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--aux-weight\",\n",
    "    type=float,\n",
    "    default=0.2,\n",
    "    help=\"Auxilary loss weight (default: 0.2)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-loss\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Semantic Encoding Loss SE-loss\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--se-weight\", type=float, default=0.2, help=\"SE-loss weight (default: 0.2)\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--midasproto\", action=\"store_true\", default=False, help=\"midasprotocol\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ignore_index\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"numeric value of ignore label in gt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--augment\",\n",
    "    action=\"store_true\",\n",
    "    default=False,\n",
    "    help=\"Use extended augmentations\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--backbone\",\n",
    "    type=str,\n",
    "    default=\"clip_vitl16_384\",\n",
    "    help=\"backbone network\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_features\",\n",
    "    type=int,\n",
    "    default=256,\n",
    "    help=\"number of featurs that go from encoder to decoder\",\n",
    ")\n",
    "parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout rate\")\n",
    "parser.add_argument(\n",
    "    \"--finetune_weights\", type=str, help=\"load weights to finetune from\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-scaleinv\",\n",
    "    default=True,\n",
    "    action=\"store_false\",\n",
    "    help=\"turn off scaleinv layers\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--no-batchnorm\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"turn off batchnorm\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead\", default=False, action=\"store_true\", help=\"wider output head\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--widehead_hr\",\n",
    "    default=False,\n",
    "    action=\"store_true\",\n",
    "    help=\"wider output head\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--arch_option\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"which kind of architecture to be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--block_depth\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"how many blocks should be used\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--activation\",\n",
    "    choices=['lrelu', 'tanh'],\n",
    "    default=\"lrelu\",\n",
    "    help=\"use which activation to activate the block\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "args.exp_name = \"lseg_ade20k_l16\"\n",
    "args.project_name = \"testing\"\n",
    "\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Use norm [0.5, 0.5, 0.5], [0.5, 0.5, 0.5] as the mean and std **\n",
      "{'base_size': 520, 'crop_size': 480}\n",
      "train\n",
      "BaseDataset: base_size 520, crop_size 480\n",
      "len(img_paths): 20210\n",
      "val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/lseg/lib/python3.9/site-packages/deprecate/deprecation.py:115: LightningDeprecationWarning: The `Accuracy` was deprecated since v1.3.0 in favor of `torchmetrics.classification.accuracy.Accuracy`. It will be removed in v1.5.0.\n",
      "  stream(template_mgs % msg_args)\n",
      "/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory checkpoints/lseg_ade20k_l16/version_0/checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:432: UserWarning: ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "# do_training(args, LSegModule)\n",
    "from modules.lseg_module import LSegModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils import make_checkpoint_callbacks, get_wandb_logger\n",
    "\n",
    "checkpoint = \"./checkpoints/demo_e200.ckpt\"\n",
    "args.lr = 0.00001\n",
    "\n",
    "lseg = LSegModule.load_from_checkpoint(checkpoint, **vars(args))\n",
    "\n",
    "# set all sorts of training parameters\n",
    "args.gpus = -1\n",
    "args.accelerator = \"ddp_spawn\"\n",
    "args.benchmark = True\n",
    "\n",
    "args.version = 0\n",
    "\n",
    "args.sync_batchnorm = True\n",
    "\n",
    "ttlogger = pl.loggers.TestTubeLogger(\n",
    "    \"checkpoints\", name=args.exp_name, version=args.version\n",
    ")\n",
    "\n",
    "args.callbacks = make_checkpoint_callbacks(args.exp_name, args.version)\n",
    "\n",
    "wblogger = get_wandb_logger(args)\n",
    "args.logger = [wblogger, ttlogger]\n",
    "\n",
    "trainer = pl.Trainer.from_argparse_args(args)\n",
    "\n",
    "# only train on a subset of data during dev\n",
    "trainer.limit_train_batches = 0.001\n",
    "trainer.limit_val_batches = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartinzwm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>checkpoints/wandb/run-20230706_153732-wb2k039z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/martinzwm/testing/runs/wb2k039z' target=\"_blank\">lseg_ade20k_l16</a></strong> to <a href='https://wandb.ai/martinzwm/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martinzwm/testing' target=\"_blank\">https://wandb.ai/martinzwm/testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martinzwm/testing/runs/wb2k039z' target=\"_blank\">https://wandb.ai/martinzwm/testing/runs/wb2k039z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'get_activation.<locals>.hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopen_clip\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lseg)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:552\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    547\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_start()\n\u001b[0;32m--> 552\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model)\n\u001b[1;32m    554\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:922\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m    921\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m    924\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m    989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:92\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:158\u001b[0m, in \u001b[0;36mDDPSpawnPlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer):\n\u001b[0;32m--> 158\u001b[0m     mp\u001b[39m.\u001b[39;49mspawn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_process, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmp_spawn_kwargs)\n\u001b[1;32m    159\u001b[0m     \u001b[39m# reset optimizers, since main process is never used for training and thus does not have a valid optim state\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     trainer\u001b[39m.\u001b[39moptimizers \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:189\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    183\u001b[0m error_queue \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mSimpleQueue()\n\u001b[1;32m    184\u001b[0m process \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mProcess(\n\u001b[1;32m    185\u001b[0m     target\u001b[39m=\u001b[39m_wrap,\n\u001b[1;32m    186\u001b[0m     args\u001b[39m=\u001b[39m(fn, i, args, error_queue),\n\u001b[1;32m    187\u001b[0m     daemon\u001b[39m=\u001b[39mdaemon,\n\u001b[1;32m    188\u001b[0m )\n\u001b[0;32m--> 189\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    190\u001b[0m error_queues\u001b[39m.\u001b[39mappend(error_queue)\n\u001b[1;32m    191\u001b[0m processes\u001b[39m.\u001b[39mappend(process)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'get_activation.<locals>.hook'"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "\n",
    "trainer.fit(lseg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def load_biomed_clip(device):\n",
    "    model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    model.to(device)\n",
    "    return model, tokenizer, preprocess_train, preprocess_val\n",
    "\n",
    "# define the LightningModule\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, lseg):\n",
    "        super().__init__()\n",
    "        self.lseg = lseg\n",
    "        # self.biomed_clip_model, self.biomed_clip_tokenizer, self.preprocess_train, self.preprocess_val = load_biomed_clip(device)\n",
    "        # image_features, text_features, logit_scale = biomed_clip_model(images, texts)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # loader_lseg = self.lseg.train_dataloader()\n",
    "        # loader_biomed_clip = None # load the biomed clip data\n",
    "\n",
    "        # return {\"lseg\": loader_lseg, \"biomed_clip\": loader_biomed_clip}\n",
    "        return self.lseg.train_dataloader()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # # access a dictionary with a batch from each DataLoader\n",
    "        # batch_lseg = batch[\"lseg\"]\n",
    "        # batch_biomed_clip = batch[\"biomed_clip\"]\n",
    "\n",
    "        # seg_loss = self.lseg.training_step(batch_lseg, batch_idx)\n",
    "        # return seg_loss\n",
    "        # # adapt_loss = adapt_loss(batch_biomed_clip)\n",
    "        return self.lseg.training_step(batch, batch_idx)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.lseg.configure_optimizers()\n",
    "\n",
    "    # def adapt_loss(self, batch_biomed_clip):\n",
    "    #     # get the image and text features from biomed clip\n",
    "    #     # get the image and text features from lseg\n",
    "    #     # compute the loss between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found output scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmartinzwm\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>checkpoints/wandb/run-20230706_063024-wb2k039z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/martinzwm/testing/runs/wb2k039z' target=\"_blank\">lseg_ade20k_l16</a></strong> to <a href='https://wandb.ai/martinzwm/testing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/martinzwm/testing' target=\"_blank\">https://wandb.ai/martinzwm/testing</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/martinzwm/testing/runs/wb2k039z' target=\"_blank\">https://wandb.ai/martinzwm/testing/runs/wb2k039z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'get_activation.<locals>.hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Model(lseg)\n\u001b[0;32m----> 3\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:552\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    547\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    548\u001b[0m )\n\u001b[1;32m    550\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mresume_start()\n\u001b[0;32m--> 552\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model)\n\u001b[1;32m    554\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:922\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m    921\u001b[0m \u001b[39m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch()\n\u001b[1;32m    924\u001b[0m \u001b[39m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_dispatch()\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mstart_predicting(\u001b[39mself\u001b[39m)\n\u001b[1;32m    989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 990\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mstart_training(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/accelerators/accelerator.py:92\u001b[0m, in \u001b[0;36mAccelerator.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_type_plugin\u001b[39m.\u001b[39;49mstart_training(trainer)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py:158\u001b[0m, in \u001b[0;36mDDPSpawnPlugin.start_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstart_training\u001b[39m(\u001b[39mself\u001b[39m, trainer):\n\u001b[0;32m--> 158\u001b[0m     mp\u001b[39m.\u001b[39;49mspawn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_process, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmp_spawn_kwargs)\n\u001b[1;32m    159\u001b[0m     \u001b[39m# reset optimizers, since main process is never used for training and thus does not have a valid optim state\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     trainer\u001b[39m.\u001b[39moptimizers \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mThis method only supports start_method=spawn (got: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mTo use a different start_method use:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[39m'\u001b[39m\u001b[39m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m start_processes(fn, args, nprocs, join, daemon, start_method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mspawn\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:189\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    183\u001b[0m error_queue \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mSimpleQueue()\n\u001b[1;32m    184\u001b[0m process \u001b[39m=\u001b[39m mp\u001b[39m.\u001b[39mProcess(\n\u001b[1;32m    185\u001b[0m     target\u001b[39m=\u001b[39m_wrap,\n\u001b[1;32m    186\u001b[0m     args\u001b[39m=\u001b[39m(fn, i, args, error_queue),\n\u001b[1;32m    187\u001b[0m     daemon\u001b[39m=\u001b[39mdaemon,\n\u001b[1;32m    188\u001b[0m )\n\u001b[0;32m--> 189\u001b[0m process\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    190\u001b[0m error_queues\u001b[39m.\u001b[39mappend(error_queue)\n\u001b[1;32m    191\u001b[0m processes\u001b[39m.\u001b[39mappend(process)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, fp)\n\u001b[1;32m     48\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/lseg/lib/python3.9/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'get_activation.<locals>.hook'"
     ]
    }
   ],
   "source": [
    "model = Model(lseg)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
